# MONAD Backend Configuration

# Model Configuration
# Default model: Phi-3 Medium 128K Instruct (Q4_K_M)
# The path will auto-resolve to: ~/Library/Application Support/ai.monad.offline/models/
MODEL_PATH=phi-3-medium-128k-instruct-q4_k_m.gguf
MODEL_CONTEXT_SIZE=4096
MODEL_N_THREADS=4

# Server Configuration
HOST=0.0.0.0
PORT=5005
DEBUG=false

# LLM Generation Parameters
MAX_TOKENS=512
TEMPERATURE=0.7
TOP_P=0.9
REPEAT_PENALTY=1.1
